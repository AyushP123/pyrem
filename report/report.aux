\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\HyPL@Entry{0<</S/r>>}
\providecommand \oddpage@label [2]{}
\providecommand\@newglossary[4]{}
\@newglossary{main}{glg}{gls}{glo}
\@newglossary{acronym}{alg}{acr}{acn}
\providecommand\@glsorder[1]{}
\providecommand\@istfilename[1]{}
\@istfilename{report.ist}
\@glsorder{word}
\citation{siegel_all_2008,cirelli_is_2008}
\citation{loomis_distribution_1938,aserinsky_regularly_1953}
\citation{toth_animal_2013}
\citation{costa-miserachs_automated_2003,sen_comparative_2014}
\HyPL@Entry{5<</S/D>>}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{intro}{{1}{1}{Introduction}{section.1}{}}
\@writefile{tdo}{\contentsline {todo}{\color@box {}{red}{\leavevmode {\color  {red}o}}\ \textbf  {QG:} conclusion should do more justice}{1}{section*.5}}
\pgfsyspdfmark {pgfid1}{4736286}{47281743}
\pgfsyspdfmark {pgfid4}{36699366}{47295199}
\pgfsyspdfmark {pgfid5}{38321382}{47050013}
\citation{chouvet_automatic_1980,haustein_automatic_1986}
\citation{langkvist_sleep_2012,sunagawa_faster:_2013}
\citation{sunagawa_faster:_2013}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \emph  {Structural description of sleep stages.} \textbf  {A}: Characteristics of the three vigilance states: \gls {nrem}, \gls {rem} and wakefulness. Typically, frequency and amplitude of the \acrfull {eeg}, and amplitude of the \acrfull {emg} are used by experts to infer vigilance state. The frequency ranges and prevalences are approximate values for healthy adult animals in normal conditions. Each wave shows a representative five second epoch with high signal to noise ratio. \textbf  {B}: Empirical transition probabilities between consecutive five second epochs. The width of the arrow is proportional to transition probability. Probabilities of remaining in each state are implied.  \relax }}{2}{figure.caption.6}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:sleep_description}{{1}{2}{\ctit {Structural description of sleep stages.} \textbf {A}: Characteristics of the three vigilance states: \gls {nrem}, \gls {rem} and wakefulness. Typically, frequency and amplitude of the \acrfull {eeg}, and amplitude of the \acrfull {emg} are used by experts to infer vigilance state. The frequency ranges and prevalences are approximate values for healthy adult animals in normal conditions. Each wave shows a representative five second epoch with high signal to noise ratio. \textbf {B}: Empirical transition probabilities between consecutive five second epochs. The width of the arrow is proportional to transition probability. Probabilities of remaining in each state are implied.  \relax }{figure.caption.6}{}}
\citation{crisler_sleep-stage_2008,ventouras_performance_2012,doroshenkov_classification_2007,pan_transition-constrained_2012,sen_comparative_2014}
\citation{crisler_sleep-stage_2008}
\citation{ventouras_performance_2012}
\citation{doroshenkov_classification_2007,pan_transition-constrained_2012}
\citation{sen_comparative_2014}
\citation{breiman_random_2001}
\citation{dietterich_machine_2002,deng_time_2013}
\citation{ding_querying_2008}
\citation{vyssotski_miniature_2006}
\citation{costa-miserachs_automated_2003}
\citation{putnam_design_1997}
\citation{vyazovskiy_nrem_2014}
\@writefile{toc}{\contentsline {section}{\numberline {2}Material and Methods}{5}{section.2}}
\newlabel{matmet}{{2}{5}{Material and Methods}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Data acquisition}{5}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Data preprocessing}{5}{subsection.2.2}}
\citation{prabhakar_application_2002}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Feature extraction from time series}{6}{subsection.2.3}}
\newlabel{sub:features}{{2.3}{6}{Feature extraction from time series}{subsection.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \emph  {Discrete wavelet decomposition.} \textbf  {A}, Through discrete wavelet transform the initial signal is split into a pair of coefficients: $cA$ and $cD$, which capture the low ($[0, f_s/2]$), and high $[f_s/2, f_s]$ frequency information, respectively. Then, discrete wavelet transform is applied iteratively on subsequent $cA$ coefficients ($cA\_1, ..., cA\_n$), thus generating ($cD\_1, ..., cD\_n$). In this example, decomposition is performed up to the sixth level ($n=6$). \textbf  {B}, Ten minutes representing the \gls {eeg} of a transition between an awake episode and a slow wave sleep(NREM) are shown for the raw wave and for each of the coefficients that are kept for feature extraction (blue rectangles in A). The variation of amplitude in each coefficient corresponds to a concomitant variation of amplitude in its frequency range. In this example, the general increase in power in the raw wave corresponds to an increase of power in the frequency bands below 32Hz, but with a decrease in power above 64Hz. In this figure, the signals with $f_s > 8$ (first five rows) are too dense to display every point. Instead, only local range(blue), inter-quantile range (orange) and median (black line) are represented.  \relax }}{6}{figure.caption.7}}
\newlabel{fig:dwd}{{2}{6}{\ctit {Discrete wavelet decomposition.} \textbf {A}, Through discrete wavelet transform the initial signal is split into a pair of coefficients: $cA$ and $cD$, which capture the low ($[0, f_s/2]$), and high $[f_s/2, f_s]$ frequency information, respectively. Then, discrete wavelet transform is applied iteratively on subsequent $cA$ coefficients ($cA\_1, ..., cA\_n$), thus generating ($cD\_1, ..., cD\_n$). In this example, decomposition is performed up to the sixth level ($n=6$). \textbf {B}, Ten minutes representing the \gls {eeg} of a transition between an awake episode and a slow wave sleep(NREM) are shown for the raw wave and for each of the coefficients that are kept for feature extraction (blue rectangles in A). The variation of amplitude in each coefficient corresponds to a concomitant variation of amplitude in its frequency range. In this example, the general increase in power in the raw wave corresponds to an increase of power in the frequency bands below 32Hz, but with a decrease in power above 64Hz. In this figure, the signals with $f_s > 8$ (first five rows) are too dense to display every point. Instead, only local range(blue), inter-quantile range (orange) and median (black line) are represented.  \relax }{figure.caption.7}{}}
\citation{hjorth_eeg_1970}
\citation{petrosian_kolmogorov_1995}
\citation{higuchi_approach_1988}
\citation{richman_physiological_2000}
\citation{dietterich_machine_2002}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \emph  {List of computed features.} For clarity, the 16 calculated features were classified into four functional families. For all epochs, all features were computed on all wavelet coefficients and raw (\gls {eeg} and \gls {emg}) signals. Therefore, a total of $16 \times (1+7 + 1 + 3) = 192$ features were generated for each five seconds of recording. The mathematical detail of the algorithms is provided in the documentation of \texttt  {pyrem}{} (see appendix). \relax }}{7}{table.caption.8}}
\newlabel{tab:features}{{1}{7}{\ctit {List of computed features.} For clarity, the 16 calculated features were classified into four functional families. For all epochs, all features were computed on all wavelet coefficients and raw (\gls {eeg} and \gls {emg}) signals. Therefore, a total of $16 \times (1+7 + 1 + 3) = 192$ features were generated for each five seconds of recording. The mathematical detail of the algorithms is provided in the documentation of \pr {} (see appendix). \relax }{table.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Addition of temporal features}{7}{subsection.2.4}}
\newlabel{eq:tau}{{1}{7}{Addition of temporal features}{equation.2.1}{}}
\newlabel{eq:window}{{2}{7}{Addition of temporal features}{equation.2.2}{}}
\citation{ding_querying_2008}
\citation{boulesteix_overview_2012}
\citation{breiman_random_2001}
\citation{r_core_team_r:_2014}
\citation{liaw_classification_2002}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Stratified Cross Validation and sampling}{8}{subsection.2.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Random forests analysis}{8}{subsection.2.6}}
\newlabel{eq:entropy}{{3}{8}{Random forests analysis}{equation.2.3}{}}
\citation{bao_pyeeg:_2011}
\citation{higuchi_approach_1988}
\citation{richman_physiological_2000}
\citation{cribari-neto_beta_2009}
\citation{bates_lme4:_2014}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Performance assessments}{9}{subsection.2.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8}Statistical analysis}{9}{subsection.2.8}}
\citation{r_core_team_r:_2014}
\citation{bao_pyeeg:_2011}
\citation{richman_physiological_2000}
\citation{bao_pyeeg:_2011}
\citation{petrosian_kolmogorov_1995}
\citation{bao_pyeeg:_2011}
\citation{bao_pyeeg:_2011}
\citation{hjorth_eeg_1970}
\citation{higuchi_approach_1988}
\citation{richman_physiological_2000}
\citation{richman_physiological_2000}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{11}{section.3}}
\newlabel{results}{{3}{11}{Results}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}A new efficient \texttt  {python} package}{11}{subsection.3.1}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \emph  {Performance improvements over \texttt  {PyEEG}.} In order to improve performance, modifications of the algorithms implemented in \texttt  {PyEEG} were carried out. This table compares how long, on average, each algorithm would take, for a random sequence of length $1280$ (\emph  {i.e.}{} $5s$ at $256$Hz). It also represents how many added points would lead to a tenfold runtime increase. For the tested range ($n \in [1280;7680] $), all algorithms add approximately an exponential time complexity ($10^{O(n)}$, $R^2 > 0.95$, for all). Several mathematical inconsistencies were also discovered and corrected. The rightmost column (\textbf  {\textdagger }) indicates whether the original implementation was corrected in order to match mathematical definition. Each alteration is mathematically justified in the section \texttt  {pyrem.univariate} of the \texttt  {pyrem}{} documentation (see appendix). \textbf  {(-)}: indicates a worse performance of \texttt  {pyrem}{} over \texttt  {PyEEG}{}. Significance levels: $^{***}$, $p-value < 10^{-3}$; $^{**}$, $p-value < 10^{-2}$, see Material and Methods for detail about statistical analysis.  \relax }}{11}{table.caption.9}}
\newlabel{tab:benchmark}{{2}{11}{\ctit {Performance improvements over \texttt {PyEEG}.} In order to improve performance, modifications of the algorithms implemented in \texttt {PyEEG} were carried out. This table compares how long, on average, each algorithm would take, for a random sequence of length $1280$ (\ie {} $5s$ at $256$Hz). It also represents how many added points would lead to a tenfold runtime increase. For the tested range ($n \in [1280;7680] $), all algorithms add approximately an exponential time complexity ($10^{O(n)}$, $R^2 > 0.95$, for all). Several mathematical inconsistencies were also discovered and corrected. The rightmost column (\textbf {\textdagger }) indicates whether the original implementation was corrected in order to match mathematical definition. Each alteration is mathematically justified in the section \texttt {pyrem.univariate} of the \pr {} documentation (see appendix). \textbf {(-)}: indicates a worse performance of \pr {} over \pyeeg {}. Significance levels: $^{***}$, $p-value < 10^{-3}$; $^{**}$, $p-value < 10^{-2}$, see Material and Methods for detail about statistical analysis.  \relax }{table.caption.9}{}}
\citation{walt_numpy_2011}
\citation{menze_comparison_2009}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Twenty variables can generate accurate predictions}{12}{subsection.3.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \emph  {Recursive variable elimination.} Random forests were recursively fitted and, at each step, only the most important $p/1.5$ variables were kept. Stratified cross-validation (see Material and Methods) was computed at each iteration. This procedure was replicated five time. The dashed blue line ($y=0.12$) indicates the median cross-validation error when all variable are used.  \relax }}{13}{figure.caption.10}}
\newlabel{fig:variable_elimination}{{3}{13}{\ctit {Recursive variable elimination.} Random forests were recursively fitted and, at each step, only the most important $p/1.5$ variables were kept. Stratified cross-validation (see Material and Methods) was computed at each iteration. This procedure was replicated five time. The dashed blue line ($y=0.12$) indicates the median cross-validation error when all variable are used.  \relax }{figure.caption.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces \emph  {Relative variable importance of the 21 selected features.} Random forest algorithm can produce a value to quantify variable importance. Variable importance corresponds to how much, statistically, a variable contributes to reducing the prediction inacuracy (or, to be more precise, the Gini impurity). Starting from 164 variables, the least important variables were recursively eliminated. This table represents the 21 most important remaining features. Features from both \gls {eeg} and \gls {emg} are important for accurate prediction. \relax }}{13}{table.caption.11}}
\newlabel{tab:importances}{{3}{13}{\ctit {Relative variable importance of the 21 selected features.} Random forest algorithm can produce a value to quantify variable importance. Variable importance corresponds to how much, statistically, a variable contributes to reducing the prediction inacuracy (or, to be more precise, the Gini impurity). Starting from 164 variables, the least important variables were recursively eliminated. This table represents the 21 most important remaining features. Features from both \gls {eeg} and \gls {emg} are important for accurate prediction. \relax }{table.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Including temporal information improves prediction}{14}{subsection.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Structural differences with ground truth}{14}{subsection.3.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \emph  {Integration of temporal information.} In order to improve prediction accuracy, information about the past and future features was added to the original features following two different strategies. Convolution inflates $p$ by adding the mean features of neighbouring points over different window sizes (eq.\nobreakspace  {}\ref  {eq:window}). The numbers in braces represent the window sizes. Alternatively, the features of the $\tau $ previous and future neighbours was added to the variables (eq.\nobreakspace  {}\ref  {eq:tau}). The dotted line represents a cross-validation error of $10\%$. All additions of temporal features reduce significantly the cross-validation error compared to the original set of features ($p-value < 5.10^{-3}$, for all, repeated t-tests). Significance threshold, $\alpha = 0.05$, after Bonferroni correction is $\alpha ' = 0.05/10 = 5.10^{-3}$.  \relax }}{15}{figure.caption.12}}
\newlabel{fig:temporal_integration}{{4}{15}{\ctit {Integration of temporal information.} In order to improve prediction accuracy, information about the past and future features was added to the original features following two different strategies. Convolution inflates $p$ by adding the mean features of neighbouring points over different window sizes (eq.~\ref {eq:window}). The numbers in braces represent the window sizes. Alternatively, the features of the $\tau $ previous and future neighbours was added to the variables (eq.~\ref {eq:tau}). The dotted line represents a cross-validation error of $10\%$. All additions of temporal features reduce significantly the cross-validation error compared to the original set of features ($p-value < 5.10^{-3}$, for all, repeated t-tests). Significance threshold, $\alpha = 0.05$, after Bonferroni correction is $\alpha ' = 0.05/10 = 5.10^{-3}$.  \relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \emph  {Structural differences.} Three metrics describing structure of sleep were computed for both ground truth (grey) and predicted (red) time series. \textbf  {A}, No significant difference in state prevalence was found. \textbf  {B}, The number of events was significantly over-estimated by the classifier for wake state and under-estimated for \gls {rem} state ($p-value < 10^{-15}$ for both). \textbf  {C}, The average duration of wake and \gls {rem} episodes were under-estimated ($p-value < 10^{-2}$) and marginally over-estimated ($p-value = 0.057$), respectively. Log scale was used for the response variables in A and C. $n = 12$ per combination of factors. Statistical methods are detailed in the Material and Methods section. Significance levels: $^{***}$, $p-value < 10^{-3}$; $^{**}$, $p-value < 10^{-2}$.  \relax }}{15}{figure.caption.13}}
\newlabel{fig:struct_assess}{{5}{15}{\ctit {Structural differences.} Three metrics describing structure of sleep were computed for both ground truth (grey) and predicted (red) time series. \textbf {A}, No significant difference in state prevalence was found. \textbf {B}, The number of events was significantly over-estimated by the classifier for wake state and under-estimated for \gls {rem} state ($p-value < 10^{-15}$ for both). \textbf {C}, The average duration of wake and \gls {rem} episodes were under-estimated ($p-value < 10^{-2}$) and marginally over-estimated ($p-value = 0.057$), respectively. Log scale was used for the response variables in A and C. $n = 12$ per combination of factors. Statistical methods are detailed in the Material and Methods section. Significance levels: $^{***}$, $p-value < 10^{-3}$; $^{**}$, $p-value < 10^{-2}$.  \relax }{figure.caption.13}{}}
\citation{pang_unexpected_2009}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces \emph  {Relative confusion matrix.} Agreement between reference scoring (rows) and proposed classifier (column). For clarity, only the percentage of all epochs is shown. The total number of epochs was 202171. $a$, PPV (Positive Predictive Value = 1 - False Detection Rate); $b$, Overall Accuracy.  \relax }}{16}{table.caption.14}}
\newlabel{tab:confus}{{4}{16}{\ctit {Relative confusion matrix.} Agreement between reference scoring (rows) and proposed classifier (column). For clarity, only the percentage of all epochs is shown. The total number of epochs was 202171. $a$, PPV (Positive Predictive Value = 1 - False Detection Rate); $b$, Overall Accuracy.  \relax }{table.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Attribution of confidence score to predictions}{16}{subsection.3.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \emph  {A posteriori confidence assessment.} \textbf  {A}, Relation between the confidence value derived from proportions of votes (eq.\ref  {eq:entropy}) and actual proportion of error. Cross-validation error decreases with empirical confidence value. For low values of confidence, $[0, 0.1]$, the predictor is not very reliable (more that $45\%$ error, whislt chance would be $67\%$). In contrast, within the highest confidence range, $(0.9, 1.0]$, misclassification are very rare($<0.5 \%$). \textbf  {B}, Distribution of confidence values for all epochs. The overall median confidence value is at 0.69. \textbf  {C}, Visualisation of approximately 30 minutes of representative \gls {eeg} and \gls {emg} recording. The variation in the most important \gls {eeg} variable, mean power in the cD\_6 sub-band, is also plotted. Both predicted and ground truth states are colour coded (red: awake, blue: \gls {rem}, green: \gls {nrem}). The doubt level ($1 - c$) associated with predictions is displayed on top of the prediction annotations.  \relax }}{17}{figure.caption.15}}
\newlabel{fig:error}{{6}{17}{\ctit {A posteriori confidence assessment.} \textbf {A}, Relation between the confidence value derived from proportions of votes (eq.\ref {eq:entropy}) and actual proportion of error. Cross-validation error decreases with empirical confidence value. For low values of confidence, $[0, 0.1]$, the predictor is not very reliable (more that $45\%$ error, whislt chance would be $67\%$). In contrast, within the highest confidence range, $(0.9, 1.0]$, misclassification are very rare($<0.5 \%$). \textbf {B}, Distribution of confidence values for all epochs. The overall median confidence value is at 0.69. \textbf {C}, Visualisation of approximately 30 minutes of representative \gls {eeg} and \gls {emg} recording. The variation in the most important \gls {eeg} variable, mean power in the cD\_6 sub-band, is also plotted. Both predicted and ground truth states are colour coded (red: awake, blue: \gls {rem}, green: \gls {nrem}). The doubt level ($1 - c$) associated with predictions is displayed on top of the prediction annotations.  \relax }{figure.caption.15}{}}
\citation{bao_pyeeg:_2011}
\citation{bezanson_julia:_2012}
\citation{morin_shining_2012,crick_can_2014}
\citation{sen_comparative_2014}
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion}{19}{section.4}}
\newlabel{discussion}{{4}{19}{Discussion}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Software package for feature computation}{19}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Exhaustive feature extraction}{19}{subsection.4.2}}
\citation{crisler_sleep-stage_2008}
\citation{breiman_random_2001}
\citation{doroshenkov_classification_2007,pan_transition-constrained_2012}
\citation{dietterich_machine_2002}
\citation{deng_time_2013}
\citation{breiman_random_2001}
\citation{sen_comparative_2014}
\citation{strobl_conditional_2008}
\citation{ebrahimi_automatic_2008,chapotot_automated_2010,sen_comparative_2014}
\citation{ding_querying_2008}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Random forest classification}{21}{subsection.4.3}}
\citation{costa-miserachs_automated_2003}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Rigorous and comprehensive model evaluation}{22}{subsection.4.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Quality of the raw data}{22}{subsection.4.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Overall results}{23}{subsection.4.6}}
\bibdata{report.bib}
\bibcite{siegel_all_2008}{1}
\bibcite{cirelli_is_2008}{2}
\bibcite{loomis_distribution_1938}{3}
\bibcite{aserinsky_regularly_1953}{4}
\bibcite{toth_animal_2013}{5}
\bibcite{costa-miserachs_automated_2003}{6}
\bibcite{sen_comparative_2014}{7}
\bibcite{chouvet_automatic_1980}{8}
\bibcite{haustein_automatic_1986}{9}
\bibcite{langkvist_sleep_2012}{10}
\bibcite{sunagawa_faster:_2013}{11}
\bibcite{crisler_sleep-stage_2008}{12}
\bibcite{ventouras_performance_2012}{13}
\bibcite{doroshenkov_classification_2007}{14}
\bibcite{pan_transition-constrained_2012}{15}
\bibcite{breiman_random_2001}{16}
\bibcite{dietterich_machine_2002}{17}
\bibcite{deng_time_2013}{18}
\bibcite{ding_querying_2008}{19}
\bibcite{vyssotski_miniature_2006}{20}
\bibcite{putnam_design_1997}{21}
\bibcite{vyazovskiy_nrem_2014}{22}
\bibcite{prabhakar_application_2002}{23}
\bibcite{hjorth_eeg_1970}{24}
\bibcite{petrosian_kolmogorov_1995}{25}
\bibcite{higuchi_approach_1988}{26}
\bibcite{richman_physiological_2000}{27}
\bibcite{boulesteix_overview_2012}{28}
\bibcite{r_core_team_r:_2014}{29}
\bibcite{liaw_classification_2002}{30}
\bibcite{bao_pyeeg:_2011}{31}
\bibcite{cribari-neto_beta_2009}{32}
\bibcite{bates_lme4:_2014}{33}
\bibcite{walt_numpy_2011}{34}
\bibcite{menze_comparison_2009}{35}
\bibcite{pang_unexpected_2009}{36}
\bibcite{bezanson_julia:_2012}{37}
\bibcite{morin_shining_2012}{38}
\bibcite{crick_can_2014}{39}
\bibcite{strobl_conditional_2008}{40}
\bibcite{ebrahimi_automatic_2008}{41}
\bibcite{chapotot_automated_2010}{42}
\bibstyle{ieeetr}
\@writefile{toc}{\contentsline {section}{Appendices}{30}{section*.19}}
\@writefile{toc}{\contentsline {section}{\numberline {A}\texttt  {pyrem}{} documentation}{30}{Appendix.a.A}}
