\section{Results} \label{results}

\subsection{A new efficient \texttt{python} package}
Several algorithms to extract features from univariate time series had already been implemented in the \py{} package \pyeeg{}\citationneeded{}.
Unfortunately, some of them were critically slow, and could therefore not realistically have been used in the present study.
Preliminary investigation of \pyeeg{} source code revealed that runtimes may be improved by vectorising expressions and pre-allocating of temporary arrays.
Therefore, systematic reimplementation of all algorithms in \pyeeg{} was undertaken.
Very significant improvement in performance were achieved for almost all functions(table~\ref{tab:benchmark}).
Critically, sample \citationneeded{} and absolute \citationneeded{} became usable in reasonable time.
\input{./tables/benchmark}

Importantly, several mathematical inconsistencies between the original code and the mathematical definitions were also noticed.
This affected five of the eight reimplemented functions(table~\ref{tab:benchmark}).
Detail of the corrections performed are provided, as notes, in the documentation of the new package (see appendix).
Numerical results for the three other functions were consistent throughout optimisation.

In order to facilitate feature extraction, several data structures and routines were also implemented
in a new python package named \pr{}.
Briefly, extensions of \texttt{numpy} arrays providing meta-data, sampling frequency, and other attributes were used to represent time series.
User friendly indexing with string representing time was also developed.
In addition, a container for time series of discrete annotation levels, each linked to a confidence level, was built.
Importantly, a container for multiple time series, which supports heterogeneous (between time series) sampling frequencies was implemented.
The new package also provides visualisation, input/output, and wrappers for resampling and discrete wavelet decomposition.
Finally, unittests were implemented to ensure persistence of mathematical and programmatic validity throughout developmental stage.
A complete documentation of \pr{} is provided in the appendix of the report herein.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Twenty variables can generate accurate predictions}
Including temporal information (see next section) results in multiplication the number of variable, rendering computation difficult, and prediction potentially less accurate.
Therefore, iterative elimination of variables based on their importance was undertaken.
Starting with all 164 variables, random forests were trained, and the number of features was reduced by a factor $1.3$  by eliminating the least important variables.
For each iteration, the stratified cross-validation error (see material and methods) was computed (fig.~\ref{fig:variable_elimination}).

\input{./figures/variable_elimination}
The predictive accuracy globally increases with the number of variables.
However, this increase is very moderate for ($p>8$) this indicates that dimensionality can be reduced without largely impacting accuracy.
For further investigation, $p=20$ was considered to be a good compromise between error and computational load.
\input{./tables/importances}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Including temporal information improves prediction}
Manual scorer usually use contextual information in order to make a decision concerning a given state.
For instance, implicit assumptions are made on the temporal consistency of the states.
Therefore, it seems important to account for contextual information when building a predictor.
In this study, two different approaches were pursued.
Either the local means of features over different ranges were added to the features
of each epochs (eq.~\ref{eq:window}),
or the features of neighbouring time points were included (eq.~\ref{eq:tau}).

A comparison of both approaches is proposed in figure \ref{fig:temporal_integration}.
Significant improvement was achieved by both methods by including even little temporal information ($\tau = 1$, $n=\{1,3\}$).
The best accuracy was achieved with $\tau = 2$.
This value is also a good compromise in so far as it only inflates $p$ by five fold.
In addition, it is advantageous to require only five contiguous neighbouring epochs instead of having to integrate minutes of contextual information.
Combining both approaches did not improve prediction any further (data not shown).

\input{./figures/temporal_integration}


\subsection{Structural differences with ground truth}

\input{./figures/struct_assess}

After selecting important variables and defining new ones, general and structural performance of the classifier was assessed.
For this purpose, random forests were trained on samples accounting for the unbalanced prevalence.
Predictions were generated using the same stratified sampling approach.
That is, predictions on each time series were made by a classifier for which
the training set did not contain any point from this time series.


\input{./tables/confus}

The global confusion matrix is presented in table \ref{tab:confus}. The overall accuracy is 0.92.
For \gls{nrem} and wake, both specificity and positive predictive value were above 0.90. However
for \gls{rem} epochs, the specificity is only 0.74, and the false detection rate is 15\%.

In order to investigate the structural differences between ground truth and the predicted states,
three metrics describing physiological properties of sleep were computed (fig.~\ref{fig:struct_assess}).

Prevalence(fig.~\ref{fig:struct_assess}A) of states appears to be the most widely used metric to describe sleep patterns\citationneeded{}.
No significant difference was found between the ground truth and predicted prevalences ($p-value > 0.13$ for all, z test on the interaction terms of $\beta$ regression).

Sleep fragmentation is often assessed by a combination of two variables: number of episodes of a given state (fig.~\ref{fig:struct_assess}B) and average duration of all episodes per state (fig.~\ref{fig:struct_assess}C) \citationneeded{}.

Large differences were observed between the number of events computed from the ground truth or the predicted data.
($p-value < 10^{-15}$ for all, t test on the interaction terms of Poison GLMM).

Finally, significant differences were found between methods for the average durations of \gls{rem} ($p-value < 10^{-4}$) an wake ($p-value < 10^{-3}$) episodes
(t test on the interaction terms of linear mixed model).

\subsection{Attribution of confidence score to predictions}
Since classification can be inaccurate, it would be interesting to associate `confidence' score to each prediction.
A entropy based confidence metric $c$ (eq. \ref{eq:entropy}) was defined for this purpose.
In order to validate it, the average cross-validation error was computed for different degrees of confidence (fig\ref{fig:error}A).

\input{./figures/error}

As expected, the probability of misclassification decreases monotonically with $c$.
In addition, error rate seem to tend to zero when the confidence value is one, and, for confidences close to zero, the predictor is very inaccurate.
These characteristics indicate that $c$ can be used to as a supporting value for predictions.
One application of such a confidence level could be to provide user with an overall quality assessment.
In addition, it  makes it possible possible for the used to display while visually inspecting a recording (fig\ref{fig:error}C).
